
from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings
from flair.data import Sentence
import numpy as np
import collections
import yake
from numpy.linalg import norm

class CAKE:
    def __init__(self, model_path = None, tokenizer = None, label_names = None, 
                 label_descriptions = None, input_docs = None, input_labels = None, input_docs_test = None):
        if model_path == None:
            print('You have to define a model!')
            #raise exception
        else:
            self.model_path = model_path
        if tokenizer == None:
            print('You have to define a tokenizer!')
            #raise exception
        else:
            self.tokenizer = tokenizer
        if label_names == None:
            print('You have to define label names!')
            #raise exception
        else:
            self.label_names = label_names
        if label_descriptions == None:
            print('You have to define label descriptions!')
            #raise exception
        else:
            self.label_descriptions = label_descriptions
        if input_docs == None:
            print('You have to define training docs!')
            #raise exception
        else:
            self.input_docs = input_docs
        if input_labels == None:
            print('You have to define training labels!')
            #raise exception
        else:
            self.input_labels = input_labels
        if input_docs_test == None:
            print('You have to define training labels!')
            #raise exception
        else:
            self.input_docs_test = input_docs_test
        self.embedding_doc = TransformerDocumentEmbeddings(model_path, layers='-1')
        self.embedding_word = TransformerWordEmbeddings(model_path, layers='-1')
        self.label_1()
        self.label_2()
        self.label_2_doc()
        self.label_3()
        
    def label_1(self):
        """
        Label embedding is the average of the document embeddings of its associated documents
        """
        label_embeddings = []
        for label in self.label_names:
            label_embeddings.append([])
        for idx, doc in enumerate(self.input_docs):
            sentence = Sentence(doc)
            sentence = self.embedding_doc.embed(sentence)
            for token in sentence:
                doc_emb = np.array(token.embedding.cpu())
            for idl, label in enumerate(self.input_labels[idx]):
                if label == 1:
                    label_embeddings[idl].append(doc_emb)
        self.embedding_label_1 = []
        for idl, label in enumerate(self.label_names):
            self.embedding_label_1.append(np.array(label_embeddings[idl]).mean(axis=0))
        self.embedding_label_1 = np.array(self.embedding_label_1)
       
    def label_2(self):
        """
        Label embedding is taken by using a description generated by chat gpt or by a human expert (ideally)
        NOTE: we only keep the word embeddin of the label, rather than the embedding of the description
        """
        label_embeddings = []
        label_embedding = []
        for idl, description in enumerate(self.label_descriptions):
            sentence = Sentence(description)
            self.embedding_word.embed(sentence)
            temp_label = []
            for token in sentence[:len(self.label_names[idl].split(' '))]: #take only the splits
                temp_label.append(list(np.array(token.embedding.cpu())))
            label_embeddings.append(np.mean(temp_label,axis=0))
        self.embedding_label_2 = np.array(label_embeddings)
        
    def label_2_zero(self, label, description):
        """
        Label embedding is taken by using a description generated by chat gpt or by a human expert (ideally)
        NOTE: we only keep the word embeddin of the label, rather than the embedding of the description
        """
        sentence = Sentence(description)
        self.embedding_word.embed(sentence)
        temp_label = []
        for token in sentence[:len(label.split(' '))]: #take only the splits
            temp_label.append(list(np.array(token.embedding.cpu())))
        return np.mean(temp_label,axis=0)


    def label_2_doc(self):
        """
        Label embedding is taken by using a description generated by chat gpt or by a human expert (ideally)
        NOTE: we only keep the word embeddin of the label, rather than the embedding of the description
        """
        label_embeddings = []
        for idl, description in enumerate(self.label_descriptions):
            sentence = Sentence(description)
            sentence = self.embedding_doc.embed(sentence)
            for token in sentence:
                label_embeddings.append(list(np.array(token.embedding.cpu())))
        self.embedding_label_2_doc = np.array(label_embeddings)
        
    def label_2_doc_zero(self, label, description):
        """
        Label embedding is taken by using a description generated by chat gpt or by a human expert (ideally)
        NOTE: we only keep the word embeddin of the label, rather than the embedding of the description
        """
        sentence = Sentence(description)
        sentence = self.embedding_doc.embed(sentence)
        for token in sentence:
            return np.array(list(np.array(token.embedding.cpu())))
    
    def label_3(self):
        """
        Label embedding is take by using a pseudo-sentence in conjunction to the examined article.
        NOTE: we only keep the embedding of the label rather than for the whole sentence
        NOTE 2: Each document has a different embedding for the label (to be changed in the future to also do this procedure
                                                                      for each new incoming instance rather than the test set) 
        """
        local_label_embeddings = []
        for document in self.input_docs_test:
            doc_label_emb = []
            for label  in range(len(self.label_names)):
                pseudo_sentence = self.label_names[label]+" label describes document: " + document
                sentence = Sentence(pseudo_sentence)
                self.embedding_word.embed(sentence)
                temp_label = []
                for token in sentence[:len(self.label_names[label].split(' '))]: #take only the splits
                    #print(token)
                    temp_label.append(list(np.array(token.embedding.cpu())))
                doc_label_emb.append(np.mean(temp_label,axis=0))
            local_label_embeddings.append(doc_label_emb)
        self.embedding_label_3 = np.array(local_label_embeddings)
        
    def label_3_zero(self, doc, label):
        """
        Label embedding is take by using a pseudo-sentence in conjunction to the examined article.
        NOTE: we only keep the embedding of the label rather than for the whole sentence
        NOTE 2: Each document has a different embedding for the label (to be changed in the future to also do this procedure
                                                                      for each new incoming instance rather than the test set) 
        """
        pseudo_sentence = label+" label describes document: " + doc
        sentence = Sentence(pseudo_sentence)
        self.embedding_word.embed(sentence)
        temp_label = []
        for token in sentence[:len(label.split(' '))]: #take only the splits
            #print(token)
            temp_label.append(list(np.array(token.embedding.cpu())))
        return np.mean(temp_label,axis=0)

    
    def keyphrase_1(self, keyphrases):
        """
        Keyphrase embedding is the document embedding returned by the trained transformer when giving the keyphrase as input
        NOTE: Works for every keyphrase extraction technique
        """
        ck_embs = []
        for keyphrase in keyphrases:
            sentence = Sentence(keyphrase)
            sentence = self.embedding_doc.embed(sentence)
            for token in sentence:
                ck_embs.append(list(np.array(token.embedding.cpu())))
        ck_embs = np.array(ck_embs)
        return keyphrases, ck_embs

    def keyphrase_2(self, input_text, n_gramsize = [2]):
        """
        We embed the whole document, then take the embeddings of keyphrase as it appears inside the document.
        NOTE: works only with the n-grams keyphrase extraction technique
        NOTE 2: if the keyphrase appears multiple times then we average the embeddings of its appearances
        NOTE 3: Works for different n-gram sizes (we will experiment with 1,2,3)
        """
        embedding = TransformerWordEmbeddings('./Trained Models/bert_hx',layers='-1')
        sentence = Sentence(input_text)
        self.embedding_word.embed(sentence)
        cks = []
        ck_embs = []
        temp_cks =[]
        temp_ck_embs = []
        for token in sentence: # take only the keyphrases words
            cks.append(token.text)
            ck_embs.append(list(np.array(token.embedding.cpu())))
        for j in n_gramsize:
            ngrams = zip(*[cks[i:] for i in range(j)])
            temp_cks.extend([" ".join(ngram) for ngram in ngrams])
            ngrams = zip(*[ck_embs[i:] for i in range(j)])
            temp_ck_embs.extend([np.array(ngram).mean(axis=0) for ngram in ngrams])
        return temp_cks, np.array(temp_ck_embs)

    def keyphrase_3(self, keyphrases, input_text):
        """
        The keyphrase embedding is generated using a pseudo-sentence in conjunction to the examined document
        NOTE: we only keep the embedding of the first appearance of the keyphrase (before the pseudo-sentence)
        NOTE 2: works for every keyphrase extraction technique
        """
        ck_embs = []
        for keyphrase in keyphrases:
            pseudo_sentence = keyphrase + " is a keyphrase for document: " + input_text
            sentence = Sentence(pseudo_sentence)
            self.embedding_word.embed(sentence)
            temp_emb = []
            for token in sentence[0:len(keyphrase.split(' '))]: # take only the keyphrases words
                temp_emb.append(list(np.array(token.embedding.cpu())))
            ck_embs.append(np.mean(temp_emb,axis=0))
        ck_embs = np.array(ck_embs)
        return keyphrases, ck_embs
    
    def matching(self, text, keyphrases, keyphrase_weights, width_size):
        """
        Matches the keyphrases with their feature importance scores to the original document,
        in order to produce feature importance interpretations (in word level)
        """
        tokens = self.tokenizer.tokenize(text.lower()) #Here use the model's tokenizer
        interpretations = [[0 for i in range(len(tokens))] for i in self.label_names]
        found_ratio = 0
        keyphrase_ratio = 0
        for idk, keyphrase in enumerate(keyphrases):
            flag = False
            for label in range(len(self.label_names)):
                if keyphrase_weights[label][idk] != 0:
                    flag = True
            if flag:
                keyphrase_ratio = keyphrase_ratio + 1
                keyphrase_tokens = self.tokenizer.tokenize(keyphrase.lower()) #Here use the model's tokenizer
                word_frequencies = dict(collections.Counter(tokens)) #We will need this to stop the search in case we find the keyphrase, for example if we now that the word web is only appearing two times, if we find two positions of the keyphrase it will stop,

                search = True
                found = 0
                ind = 0
                width = len(keyphrase_tokens) + width_size #or 5?
                while search and ind < (len(tokens)-width+1):
                    if set(keyphrase_tokens).issubset(tokens[ind:ind+width]):
                        if(tokens[ind:ind+width].index(keyphrase_tokens[0]) <= tokens[ind:ind+width].index(keyphrase_tokens[-1])):
                            found = found + 1
                            start = tokens[ind:ind+width].index(keyphrase_tokens[0])
                            end = tokens[ind:ind+width].index(keyphrase_tokens[-1])
                            for i in range(ind+start,ind+end+1):
                                if tokens[i] in keyphrase_tokens:
                                    for label in range(len(self.label_names)):
                                        interpretations[label][i] = interpretations[label][i]+keyphrase_weights[label][idk]
                            ind = ind+start  #This is done because otherwise, if the width is large, it may find the keyphrase more than one time. Try to comment it out and check the logs

                    for keyword in keyphrase_tokens: #This will help us prevent unnecessary checks
                        if keyword in word_frequencies and found == word_frequencies[keyword]:
                            search = False
                    ind = ind + 1
                if found > 0:
                    found_ratio = found_ratio + 1
        if keyphrase_ratio != 0:
            return interpretations, found_ratio/keyphrase_ratio
        return interpretations, 0

    def keyphrase_interpretation(self, text, desired_keyphrases = 5, keyphrase_method = 1, label_method = 1, width = 0, negatives = True, tid = None):
        """
        Creates the interpretations for each label, based on the similarity of the extracted keyphrases,
        and the predicted labels.
        """
        if keyphrase_method == 2:
            cks, ck_embs = self.keyphrase_2(text, [2])
        elif keyphrase_method == [1,2]:
            cks, ck_embs = self.keyphrase_2(text, [1,2])
        else:
            kw_extractor = yake.KeywordExtractor(top=35)
            cks = kw_extractor.extract_keywords(text)
            cks = [i[0] for i in cks]
            if keyphrase_method == 1:
                cks, ck_embs = self.keyphrase_1(cks)
            else:
                cks, ck_embs = self.keyphrase_3(cks, text)
                
        label_interpretations = []
        for idl, label in enumerate(self.label_names):
            if(label_method == 1):
                label_emb = self.embedding_label_1[idl]
            elif(label_method == 2):
                label_emb = self.embedding_label_2[idl]
            elif(label_method == "2_doc"):
                label_emb = self.embedding_label_2_doc[idl]
            else:
                label_emb = self.embedding_label_3[tid][idl]
            cks_weight = []
            for i in range(0,len(cks)):
                feature_importance = np.dot(label_emb, ck_embs[i])/(norm(label_emb)*norm(ck_embs[i]))
                cks_weight.append(feature_importance)
            label_interpretations.append(cks_weight)
        
        for interpretation in label_interpretations:
            if negatives:
                sorted_interpretation = sorted(list(np.abs(interpretation)), reverse = True)
            else:
                sorted_interpretation = sorted(list(interpretation), reverse = True)
            if len(interpretation) > desired_keyphrases:
                if negatives:
                    threshold = abs(sorted_interpretation[desired_keyphrases-1])
                else:
                    threshold = sorted_interpretation[desired_keyphrases-1]
                for i in range(0,len(interpretation)):
                    if negatives:
                        if(abs(interpretation[i]) < threshold):
                            interpretation[i] = 0
                    else:
                        if(interpretation[i] < threshold):
                            interpretation[i] = 0
                        if interpretation[i] < 0:
                            interpretation[i] = 0
        
        return self.matching(text, cks, label_interpretations, width)

    def keyphrase_interpretation2(self, text, desired_keyphrases = 5, keyphrase_method = 1, label_method = 1, width = 0, negatives = True, tid = None):
        """
        Creates the interpretations for each label, based on the similarity of the extracted keyphrases,
        and the predicted labels.
        """
        if keyphrase_method == 2:
            cks, ck_embs = self.keyphrase_2(text, [2])
        elif keyphrase_method == [1,2]:
            cks, ck_embs = self.keyphrase_2(text, [1,2])
        else:
            kw_extractor = yake.KeywordExtractor(top=35)
            cks = kw_extractor.extract_keywords(text)
            cks = [i[0] for i in cks]
            if keyphrase_method == 1:
                cks, ck_embs = self.keyphrase_1(cks)
            else:
                cks, ck_embs = self.keyphrase_3(cks, text)
                
        label_interpretations = []
        for idl, label in enumerate(self.label_names):
            label_interpretation = []
            if(label_method == 1):
                label_emb = self.embedding_label_1[idl]
            elif(label_method == 2):
                label_emb = self.embedding_label_2[idl]
            elif(label_method == "2_doc"):
                label_emb = self.embedding_label_2_doc[idl]
            else:
                label_emb = self.embedding_label_3[tid][idl]
            cks_weight = []
            for i in range(0,len(cks)):
                feature_importance = np.dot(label_emb, ck_embs[i])/(norm(label_emb)*norm(ck_embs[i]))
                cks_weight.append(feature_importance)
            label_interpretations.append(cks_weight)
        
        for interpretation in label_interpretations:
            if negatives:
                sorted_interpretation = sorted(list(np.abs(interpretation)), reverse = True)
            else:
                sorted_interpretation = sorted(list(interpretation), reverse = True)
            if len(interpretation) > desired_keyphrases:
                if negatives:
                    threshold = abs(sorted_interpretation[desired_keyphrases-1])
                else:
                    threshold = sorted_interpretation[desired_keyphrases-1]
                for i in range(0,len(interpretation)):
                    if negatives:
                        if(abs(interpretation[i]) < threshold):
                            interpretation[i] = 0
                    else:
                        if(interpretation[i] < threshold):
                            interpretation[i] = 0
                        if interpretation[i] < 0:
                            interpretation[i] = 0
        
        return self.matching(text, cks, label_interpretations, width), cks, label_interpretations
    
    
    
    def keyphrase_interpretation_zero(self, text, label, desired_keyphrases = 5, keyphrase_method = 1, label_method = 2, width = 0, negatives = True, description = None):
        """
        Creates the interpretations for each label, based on the similarity of the extracted keyphrases,
        and the predicted labels.
        """
        if keyphrase_method == 2:
            cks, ck_embs = self.keyphrase_2(text, [2])
        elif keyphrase_method == [1,2]:
            cks, ck_embs = self.keyphrase_2(text, [1,2])
        else:
            kw_extractor = yake.KeywordExtractor(top=35)
            cks = kw_extractor.extract_keywords(text)
            cks = [i[0] for i in cks]
            if keyphrase_method == 1:
                cks, ck_embs = self.keyphrase_1(cks)
            else:
                cks, ck_embs = self.keyphrase_3(cks, text)

            
        if label_method == 2:
            label_emb = self.label_2_zero(label, description)
        elif label_method == '2d':
            label_emb = self.label_2_doc_zero(label, description)            
        else:
            label_emb = self.label_3_zero(text,label)
        cks_weight = []
        for i in range(0,len(cks)):
            feature_importance = np.dot(label_emb, ck_embs[i])/(norm(label_emb)*norm(ck_embs[i]))
            cks_weight.append(feature_importance)
        label_interpretations = [cks_weight]

        for interpretation in label_interpretations:
            if negatives:
                sorted_interpretation = sorted(list(np.abs(interpretation)), reverse = True)
            else:
                sorted_interpretation = sorted(list(interpretation), reverse = True)
            if len(interpretation) > desired_keyphrases:
                if negatives:
                    threshold = abs(sorted_interpretation[desired_keyphrases-1])
                else:
                    threshold = sorted_interpretation[desired_keyphrases-1]
                for i in range(0,len(interpretation)):
                    if negatives:
                        if(abs(interpretation[i]) < threshold):
                            interpretation[i] = 0
                    else:
                        if(interpretation[i] < threshold):
                            interpretation[i] = 0
                        if interpretation[i] < 0:
                            interpretation[i] = 0
        label_names = self.label_names
        self.label_names = [label]
        result = self.matching(text, cks, label_interpretations, width), cks, label_interpretations
        self.label_names = label_names
        return result